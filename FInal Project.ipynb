{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T05:29:16.964866Z",
     "start_time": "2024-05-06T05:29:16.954223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ],
   "id": "25b0f7bb0df7d4a",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T05:29:17.419895Z",
     "start_time": "2024-05-06T05:29:17.403936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_validation_split(data, k=5):\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    n = len(data)\n",
    "    fold_sizes = [n // k + (1 if i < n % k else 0) for i in range(k)]\n",
    "    current = 0\n",
    "    folds = []\n",
    "    for fold_size in fold_sizes:\n",
    "        start, stop = current, current + fold_size\n",
    "        val = data.iloc[start:stop]\n",
    "        train = pd.concat([data.iloc[:start], data.iloc[stop:]]).reset_index(drop=True)\n",
    "        folds.append((train, val))\n",
    "        current = stop\n",
    "    return folds"
   ],
   "id": "d86e68b88a3fa1db",
   "outputs": [],
   "execution_count": 183
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T05:29:17.858615Z",
     "start_time": "2024-05-06T05:29:17.839664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(z):\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def standard_scale(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_scaled = (X - mean) / std\n",
    "    return X_scaled\n"
   ],
   "id": "8a7d9ba201893f34",
   "outputs": [],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T05:29:18.252433Z",
     "start_time": "2024-05-06T05:29:18.231472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, alpha=1e-9):  # alpha is the smoothing parameter\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.parameters = {}\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            self.parameters[c] = {\n",
    "                'mean': X_c.mean(axis=0),\n",
    "                'var': X_c.var(axis=0) + self.alpha,  # Adding a small constant to the variance\n",
    "                'prior': len(X_c) / len(X)\n",
    "            }\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X.to_numpy()]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "        for c in self.classes:\n",
    "            prior = np.log(self.parameters[c]['prior'])\n",
    "            class_mean = self.parameters[c]['mean']\n",
    "            class_var = self.parameters[c]['var']\n",
    "            likelihood = -0.5 * np.sum(np.log(2 * np.pi * class_var) + (x - class_mean)**2 / class_var)\n",
    "            posteriors.append(prior + likelihood)\n",
    "        return self.classes[np.argmax(posteriors)]"
   ],
   "id": "abe4afaaa3ca1806",
   "outputs": [],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T05:29:18.641530Z",
     "start_time": "2024-05-06T05:29:18.621545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, tol=1e-6, lambda_reg=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.tol = tol\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = sigmoid(model)\n",
    "\n",
    "            dw = (1 / n_samples) * (np.dot(X.T, (y_predicted - y)) + self.lambda_reg * self.weights)\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Convergence Check\n",
    "            if np.max(np.abs(dw)) < self.tol and np.abs(db) < self.tol:\n",
    "                break\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = sigmoid(linear_model)\n",
    "        return np.round(y_predicted).astype(int)\n"
   ],
   "id": "c755766141e126c0",
   "outputs": [],
   "execution_count": 186
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T05:38:49.531257Z",
     "start_time": "2024-05-06T05:38:49.514303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class KNearestNeighbors:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Compute distances between x and all examples in the training set\n",
    "        distances = [np.sqrt(np.sum((x - x_train) ** 2)) for x_train in self.X_train]\n",
    "        # Get the k nearest samples, labels\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        # Majority vote, most common class label\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n"
   ],
   "id": "ae3c24df6b7fc561",
   "outputs": [],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T05:29:19.795745Z",
     "start_time": "2024-05-06T05:29:19.784775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    return accuracy, tp, fp, (tp, fp, tn, fn)\n",
    "def calculate_auc(y_true, y_pred):\n",
    "    \"\"\" Calculate AUC using a simplified ranking method. \"\"\"\n",
    "    pos = y_pred[y_true == 1]\n",
    "    neg = y_pred[y_true == 0]\n",
    "    n_pos = len(pos)\n",
    "    n_neg = len(neg)\n",
    "\n",
    "    # Each positive is ranked higher than each negative\n",
    "    correct_pairs = np.sum([1 for p in pos for n in neg if p > n])\n",
    "    total_pairs = n_pos * n_neg\n",
    "    auc = correct_pairs / total_pairs if total_pairs > 0 else 0\n",
    "    return auc"
   ],
   "id": "9f3384ff3f0c935a",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T05:29:20.565170Z",
     "start_time": "2024-05-06T05:29:20.527272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Defining data for the dataframe\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "data_path = os.path.join(current_dir, 'spambase.csv')\n",
    "\n",
    "data = pd.read_csv(data_path)\n"
   ],
   "id": "ddefb2851422cc4b",
   "outputs": [],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T05:29:21.304691Z",
     "start_time": "2024-05-06T05:29:21.286740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# do a 80 20 split\n",
    "# Splitting the data (80% train, 20% test)\n",
    "train_data = data.sample(frac=0.8, random_state=42)  # Randomly sample 80% of the data for training\n",
    "test_data = data.drop(train_data.index)               # The remaining 20% for testing\n"
   ],
   "id": "5c6129659341590d",
   "outputs": [],
   "execution_count": 190
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T05:29:26.052687Z",
     "start_time": "2024-05-06T05:29:22.195531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NaiveBayes()\n",
    "accuracies = []\n",
    "\n",
    "# Apply 5-fold cross-validation on the training data\n",
    "folds = cross_validation_split(train_data)\n",
    "\n",
    "for fold_idx, (train, val) in enumerate(folds):\n",
    "    X_train, y_train = train.drop('spam', axis=1), train['spam']\n",
    "    X_val, y_val = val.drop('spam', axis=1), val['spam']\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    accuracy = np.mean(predictions == y_val)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Fold {fold_idx+1} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"Average Cross-Validation Accuracy: {np.mean(accuracies):.4f}\")"
   ],
   "id": "f4c9d733578f111c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Accuracy: 0.8128\n",
      "Fold 2 Accuracy: 0.8166\n",
      "Fold 3 Accuracy: 0.7867\n",
      "Fold 4 Accuracy: 0.8234\n",
      "Fold 5 Accuracy: 0.8125\n",
      "Average Cross-Validation Accuracy: 0.8104\n"
     ]
    }
   ],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T05:32:31.730656Z",
     "start_time": "2024-05-06T05:32:09.182611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = train_data.drop('spam', axis=1)\n",
    "y_train = train_data['spam'].values\n",
    "X_test = test_data.drop('spam', axis=1)\n",
    "y_test = test_data['spam'].values\n",
    "\n",
    "# Scaling for models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize models\n",
    "naive_bayes_model = NaiveBayes()\n",
    "logistic_regression_model = LogisticRegression(learning_rate=0.01, n_iterations=1000)\n",
    "knn = KNearestNeighbors(k=5)\n",
    "\n",
    "# Train Naive Bayes and Logistic Regression on original unscaled data\n",
    "naive_bayes_model.fit(X_train, y_train)\n",
    "logistic_regression_model.fit(X_train_scaled, y_train)  # Assuming Logistic Regression can benefit from scaling\n",
    "\n",
    "# Predict using Naive Bayes and Logistic Regression\n",
    "y_pred_nb = naive_bayes_model.predict(X_test)\n",
    "y_pred_lr = logistic_regression_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics for Naive Bayes and Logistic Regression\n",
    "accuracy_nb, tp_nb, fp_nb, conf_matrix_nb = calculate_metrics(y_test, y_pred_nb)\n",
    "auc_nb = calculate_auc(y_test, y_pred_nb)\n",
    "accuracy_lr, tp_lr, fp_lr, conf_matrix_lr = calculate_metrics(y_test, y_pred_lr)\n",
    "auc_lr = calculate_auc(y_test, y_pred_lr)\n",
    "\n",
    "# Print Naive Bayes and Logistic Regression results\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(f\"Accuracy: {accuracy_nb:.4f}\")\n",
    "print(f\"True Positives: {tp_nb}\")\n",
    "print(f\"False Positives: {fp_nb}\")\n",
    "print(f\"AUC: {auc_nb:.4f}\")\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_lr:.4f}\")\n",
    "print(f\"True Positives: {tp_lr}\")\n",
    "print(f\"False Positives: {fp_lr}\")\n",
    "print(f\"AUC: {auc_lr:.4f}\")\n",
    "\n",
    "# Train and predict using KNN on scaled data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_knn, tp_knn, fp_knn, conf_matrix_knn = calculate_metrics(y_test, y_pred_knn)\n",
    "auc_knn = calculate_auc(y_test, y_pred_knn)\n",
    "\n",
    "# Print KNN results\n",
    "print(\"\\nKNN Results:\")\n",
    "print(f\"Accuracy: {accuracy_knn:.4f}\")\n",
    "print(f\"True Positives: {tp_knn}\")\n",
    "print(f\"False Positives: {fp_knn}\")\n",
    "print(f\"AUC: {auc_knn:.4f}\")"
   ],
   "id": "6eff462cc10f49b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results:\n",
      "Accuracy: 0.8337\n",
      "True Positives: 336\n",
      "False Positives: 130\n",
      "AUC: 0.7191\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.9043\n",
      "True Positives: 300\n",
      "False Positives: 29\n",
      "AUC: 0.7925\n",
      "\n",
      "KNN Results:\n",
      "Accuracy: 0.9087\n",
      "True Positives: 310\n",
      "False Positives: 35\n",
      "AUC: 0.8096\n"
     ]
    }
   ],
   "execution_count": 194
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
